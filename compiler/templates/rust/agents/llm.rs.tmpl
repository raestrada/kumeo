use std::error::Error;
use async_nats::Client as NatsClient;
use ollama_rs::{Ollama, generation::completion::{request::GenerationRequest, GenerationResponse}};
use serde::{Deserialize, Serialize};
use tokio::sync::mpsc;
use tracing::{info, error, warn};

/// Agent configuration for {{agent_id}}
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct {{agent_id}}Config {
    pub engine: String,
    pub prompt_template: String,
    pub temperature: f32,
    pub max_tokens: usize,
}

/// Event received from the input topic
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct InputEvent {
    pub id: String,
    pub data: serde_json::Value,
    pub metadata: serde_json::Value,
}

/// Event to be published to the output topic
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OutputEvent {
    pub id: String,
    pub data: serde_json::Value,
    pub metadata: serde_json::Value,
    pub response: String,
}

/// {{agent_id}} - LLM Agent for {{workflow_name}}
/// Generated by Kumeo compiler
pub struct {{agent_id}}Agent {
    nats_client: NatsClient,
    ollama_client: Ollama,
    config: {{agent_id}}Config,
    input_topic: String,
    output_topic: String,
}

impl {{agent_id}}Agent {
    /// Create a new instance of the {{agent_id}}Agent
    pub async fn new(nats_url: &str) -> Result<Self, Box<dyn Error>> {
        let nats_client = async_nats::connect(nats_url).await?;
        let ollama_client = Ollama::default();
        
        let config = {{agent_id}}Config {
            engine: String::from("{{engine}}"),
            prompt_template: String::from("{{prompt}}"),
            temperature: 0.7,
            max_tokens: 2048,
        };
        
        Ok(Self {
            nats_client,
            ollama_client,
            config,
            input_topic: String::from("{{input_topic}}"),
            output_topic: String::from("{{output_topic}}"),
        })
    }
    
    /// Start the agent's message processing loop
    pub async fn run(&self) -> Result<(), Box<dyn Error>> {
        info!("Starting {{agent_id}} LLM agent for workflow {{workflow_name}}");
        
        // Subscribe to the input topic
        let mut subscriber = self.nats_client.subscribe(&self.input_topic).await?;
        
        // Process messages
        while let Some(msg) = subscriber.next().await {
            match self.process_message(&msg).await {
                Ok(response) => {
                    // Publish the response to the output topic
                    if let Err(e) = self.nats_client.publish(&self.output_topic, response.into()).await {
                        error!("Failed to publish response: {}", e);
                    }
                },
                Err(e) => {
                    error!("Error processing message: {}", e);
                }
            }
        }
        
        Ok(())
    }
    
    /// Process a message through the LLM
    async fn process_message(&self, msg: &async_nats::Message) -> Result<Vec<u8>, Box<dyn Error>> {
        // Parse the input event
        let input: InputEvent = serde_json::from_slice(&msg.payload)?;
        info!("Processing message with ID: {}", input.id);
        
        // Replace placeholders in the prompt template
        let input_json = serde_json::to_string_pretty(&input.data)?;
        let prompt = self.config.prompt_template
            .replace("{{input}}", &input_json);
        
        // Create the LLM request
        let request = GenerationRequest::new(self.config.engine.clone(), prompt);
        
        // Call the LLM
        let response = self.ollama_client.generate(request).await?;
        let llm_response = response.response;
        
        // Create the output event
        let output = OutputEvent {
            id: input.id,
            data: input.data,
            metadata: input.metadata,
            response: llm_response,
        };
        
        // Serialize the output event
        let output_bytes = serde_json::to_vec(&output)?;
        
        Ok(output_bytes)
    }
}
