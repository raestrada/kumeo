//! Configuration for the LLM Agent

use serde::{Deserialize, Serialize};
use std::env;
use std::fs;
use std::path::Path;
use url::Url;

/// Configuration for the LLM Agent
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LLMConfig {
    /// The LLM provider to use (e.g., "openai", "anthropic", "local")
    pub provider: String,
    
    /// The model to use (e.g., "gpt-4", "claude-2")
    pub model: String,
    
    /// API key for the LLM provider
    pub api_key: Option<String>,
    
    /// Base URL for the API (for self-hosted models)
    pub base_url: Option<String>,
    
    /// Default temperature for sampling
    #[serde(default = "default_temperature")]
    pub temperature: f32,
    
    /// Maximum number of tokens to generate
    #[serde(default = "default_max_tokens")]
    pub max_tokens: u32,
    
    /// Top-p sampling parameter
    #[serde(default = "default_top_p")]
    pub top_p: f32,
    
    /// Frequency penalty
    #[serde(default = "default_frequency_penalty")]
    pub frequency_penalty: f32,
    
    /// Presence penalty
    #[serde(default = "default_presence_penalty")]
    pub presence_penalty: f32,
    
    /// Input topic for receiving prompts
    pub input_topic: String,
    
    /// Output topic for sending responses
    pub output_topic: String,
    
    /// Error topic for error messages
    pub error_topic: String,
    
    /// Timeout for API requests in seconds
    #[serde(default = "default_timeout_secs")]
    pub timeout_secs: u64,
    
    /// Maximum retries for failed requests
    #[serde(default = "default_max_retries")]
    pub max_retries: u32,
    
    /// Whether to enable streaming responses
    #[serde(default = "default_enable_streaming")]
    pub enable_streaming: bool,
}

fn default_temperature() -> f32 {
    0.7
}

fn default_max_tokens() -> u32 {
    2048
}

fn default_top_p() -> f32 {
    1.0
}

fn default_frequency_penalty() -> f32 {
    0.0
}

fn default_presence_penalty() -> f32 {
    0.0
}

fn default_timeout_secs() -> u64 {
    30
}

fn default_max_retries() -> u32 {
    3
}

fn default_enable_streaming() -> bool {
    false
}

/// Load the agent configuration
pub fn load_config() -> LLMConfig {
    // Try to load from environment variable first
    if let Ok(config_str) = env::var("{{agent_name | upper}}_CONFIG") {
        if let Ok(mut config) = serde_json::from_str::<LLMConfig>(&config_str) {
            // Try to get API key from environment if not set
            if config.api_key.is_none() {
                if let Ok(api_key) = env::var("LLM_API_KEY") {
                    config.api_key = Some(api_key);
                }
            }
            return config;
        }
    }
    
    // Try to load from config file
    let config_path = env::var("{{agent_name | upper}}_CONFIG_FILE")
        .unwrap_or_else(|_| "config/{{agent_name | lower}}.json".to_string());
    
    if Path::new(&config_path).exists() {
        if let Ok(contents) = fs::read_to_string(&config_path) {
            if let Ok(mut config) = serde_json::from_str::<LLMConfig>(&contents) {
                // Try to get API key from environment if not set
                if config.api_key.is_none() {
                    if let Ok(api_key) = env::var("LLM_API_KEY") {
                        config.api_key = Some(api_key);
                    }
                }
                return config;
            }
        }
    }
    
    // Fall back to defaults
    LLMConfig {
        provider: "openai".to_string(),
        model: "gpt-4".to_string(),
        api_key: None,
        base_url: None,
        temperature: 0.7,
        max_tokens: 2048,
        top_p: 1.0,
        frequency_penalty: 0.0,
        presence_penalty: 0.0,
        input_topic: "llm.prompts".to_string(),
        output_topic: "llm.responses".to_string(),
        error_topic: "llm.errors".to_string(),
        timeout_secs: 30,
        max_retries: 3,
        enable_streaming: false,
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::env;
    use tempfile::tempdir;
    use std::fs::File;
    use std::io::Write;

    #[test]
    fn test_load_default_config() {
        env::remove_var("{{agent_name | upper}}_CONFIG");
        env::remove_var("{{agent_name | upper}}_CONFIG_FILE");
        env::remove_var("LLM_API_KEY");
        
        let config = load_config();
        assert_eq!(config.provider, "openai");
        assert_eq!(config.model, "gpt-4");
        assert_eq!(config.temperature, 0.7);
        assert_eq!(config.max_tokens, 2048);
        assert_eq!(config.input_topic, "llm.prompts");
        assert_eq!(config.output_topic, "llm.responses");
        assert_eq!(config.error_topic, "llm.errors");
        assert_eq!(config.timeout_secs, 30);
        assert_eq!(config.max_retries, 3);
        assert!(!config.enable_streaming);
    }
    
    #[test]
    fn test_load_config_from_env() {
        let test_config = r#"{
            "provider": "anthropic",
            "model": "claude-2",
            "temperature": 0.8,
            "max_tokens": 1024,
            "input_topic": "claude.prompts",
            "output_topic": "claude.responses",
            "error_topic": "claude.errors",
            "timeout_secs": 60,
            "max_retries": 5,
            "enable_streaming": true
        }"#;
        
        env::set_var("{{agent_name | upper}}_CONFIG", test_config);
        
        let config = load_config();
        assert_eq!(config.provider, "anthropic");
        assert_eq!(config.model, "claude-2");
        assert_eq!(config.temperature, 0.8);
        assert_eq!(config.max_tokens, 1024);
        assert_eq!(config.input_topic, "claude.prompts");
        assert_eq!(config.output_topic, "claude.responses");
        assert_eq!(config.error_topic, "claude.errors");
        assert_eq!(config.timeout_secs, 60);
        assert_eq!(config.max_retries, 5);
        assert!(config.enable_streaming);
        
        env::remove_var("{{agent_name | upper}}_CONFIG");
    }
    
    #[test]
    fn test_load_api_key_from_env() {
        env::remove_var("{{agent_name | upper}}_CONFIG");
        env::set_var("LLM_API_KEY", "test-api-key-123");
        
        let config = load_config();
        assert_eq!(config.api_key, Some("test-api-key-123".to_string()));
        
        env::remove_var("LLM_API_KEY");
    }
}
